import hashlib
import math
from io import BytesIO
from typing import Generator

import bittensor as bt
from fastapi import UploadFile
from zfec.easyfec import Decoder, Encoder

from storage_subnet.constants import (
    MAX_PIECE_SIZE,
    MIN_PIECE_SIZE,
    PIECE_LENGTH_OFFSET,
    PIECE_LENGTH_SCALING,
    EC_DATA_SIZE,
    EC_PARITY_SIZE,
)


def piece_length(
    content_length: int, min_size: int = MIN_PIECE_SIZE, max_size: int = MAX_PIECE_SIZE
) -> int:
    """Calculate the appropriate piece size based on content length.

    Args:
        content_length (int): The total length of the content.
        min_size (int, optional): The minimum allowed piece size. Defaults to MIN_PIECE_SIZE.
        max_size (int, optional): The maximum allowed piece size. Defaults to MAX_PIECE_SIZE.

    Returns:
        int: The calculated piece size within the specified bounds.
    """
    exponent = int(
        (math.log2(content_length) * PIECE_LENGTH_SCALING) + PIECE_LENGTH_OFFSET
    )
    length = 1 << exponent
    if length < min_size:
        return min_size
    elif length > max_size:
        return max_size
    return length


def piece_hash(data: bytes) -> str:
    """Calculate the SHA-1 hash of a piece of data.

    Args:
        data (bytes): The data to hash.

    Returns:
        str: The SHA-1 hash of the data.
    """
    return hashlib.sha1(data).hexdigest()


def split_file(
    file: UploadFile,
    piece_size: int,
    data_pieces: int = EC_DATA_SIZE,
    parity_pieces: int = EC_PARITY_SIZE,
) -> Generator[tuple[str, bytes, int], None, None]:
    """
    Stream data and parity pieces as they are generated.

    Args:
        file (UploadFile): The input file to process.
        piece_size (int): Size of each data piece (in bytes).
        data_pieces (int): Number of data pieces (k).
        parity_pieces (int): Number of parity pieces (m - k).

    Yields:
        Tuple[str, bytes, int]: A tuple where the first element indicates "data" or "parity",
                                the second element is the corresponding piece,
                                and the third element is the padding length.
    """
    total_pieces = data_pieces + parity_pieces
    encoder = Encoder(data_pieces, total_pieces)
    chunk_count = 0

    while True:
        chunk_count += 1
        bt.logging.trace(f"Reading chunk {chunk_count}...")
        # Read the next chunk of data
        chunk = file.file.read(data_pieces * piece_size)
        if not chunk:
            bt.logging.trace("End of file.")
            break  # End of file

        # Calculate padding length
        padlen = (data_pieces * piece_size) - len(chunk)
        if padlen > 0:
            bt.logging.trace(f"Padding chunk {chunk_count} with {padlen} bytes...")
            chunk = chunk.ljust(data_pieces * piece_size, b"\0")

        # Split the chunk into data pieces
        data_blocks = [
            chunk[i * piece_size : (i + 1) * piece_size] for i in range(data_pieces)
        ]

        # Encode to get both data and parity pieces
        encoded_blocks = encoder.encode(b"".join(data_blocks))

        # Yield data pieces
        for i in range(data_pieces):
            bt.logging.trace(f"Yielding data piece {i}...")
            yield ("data", encoded_blocks[i], padlen)

        # Yield parity pieces
        for i in range(data_pieces, total_pieces):
            bt.logging.trace(f"Yielding parity piece {i}...")
            yield ("parity", encoded_blocks[i], padlen)


def reconstruct_file(
    pieces: list[tuple[str, bytes, int]],
    data_pieces: int,
    parity_pieces: int,
) -> bytes:
    """
    Reconstruct the original file from data and parity pieces generated by `split_file`
    using the easyfec.Decoder, which expects a padlen argument and handles padding internally.

    Args:
        pieces (list): A list of tuples (ptype, piece, padlen).
                      ptype is "data" or "parity".
                      piece is the piece bytes.
                      padlen is the padding added for that chunk.
        data_pieces (int): k - number of data pieces.
        parity_pieces (int): m-k number of parity pieces.

    Returns:
        bytes: The reconstructed original file data.
    """
    k = data_pieces
    m = data_pieces + parity_pieces
    sharenums = list(range(m))

    decoder = Decoder(k, m)
    out_buf = BytesIO()
    chunk_pieces = []
    current_padlen = 0

    for _, piece, padlen in pieces:
        chunk_pieces.append(piece)
        current_padlen = padlen  # same padlen for all pieces in this chunk

        # Once we have m pieces, we can decode this chunk
        if len(chunk_pieces) == m:
            # Select any k pieces. Here we choose the first k.
            selected_pieces = chunk_pieces[:k]
            selected_sharenums = sharenums[:k]

            # Decode using easyfec.Decoder, passing padlen directly
            reconstructed_data = decoder.decode(
                selected_pieces, selected_sharenums, current_padlen
            )
            out_buf.write(reconstructed_data)

            # Reset for the next chunk
            chunk_pieces = []

    return out_buf.getvalue()
